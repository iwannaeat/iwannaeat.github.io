<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>【神经网络】卷积神经网络 | Hexo</title><meta name="description" content="软工课发的视频，在这里写个总结。"><meta name="keywords" content="神经网络"><meta name="author" content="iwannaeat"><meta name="copyright" content="iwannaeat"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="https://iwannaeat.github.io/2020/10/25/【神经网络】卷积神经网络/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="【神经网络】卷积神经网络"><meta name="twitter:description" content="软工课发的视频，在这里写个总结。"><meta name="twitter:image" content="https://i.loli.net/2020/10/24/WZcoAJR2F4u38dt.png"><meta property="og:type" content="article"><meta property="og:title" content="【神经网络】卷积神经网络"><meta property="og:url" content="https://iwannaeat.github.io/2020/10/25/【神经网络】卷积神经网络/"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="软工课发的视频，在这里写个总结。"><meta property="og:image" content="https://i.loli.net/2020/10/24/WZcoAJR2F4u38dt.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="prev" title="【机器学习】有关范数" href="https://iwannaeat.github.io/2020/11/05/【机器学习】有关范数/"><link rel="next" title="【神经网络】深度学习的数学基础" href="https://iwannaeat.github.io/2020/10/24/【神经网络】深度学习的数学基础/"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"cookieDomain":"https://iwannaeat.github.io/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Bookmark',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days'

  
}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#绪论"><span class="toc-number">1.</span> <span class="toc-text">绪论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积神经网络的应用"><span class="toc-number">1.1.</span> <span class="toc-text">卷积神经网络的应用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#传统神经网络-vs-卷积神经网络"><span class="toc-number">1.2.</span> <span class="toc-text">传统神经网络 vs 卷积神经网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#基本组成结构"><span class="toc-number">2.</span> <span class="toc-text">基本组成结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积"><span class="toc-number">2.1.</span> <span class="toc-text">卷积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#池化"><span class="toc-number">2.2.</span> <span class="toc-text">池化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#全连接"><span class="toc-number">2.3.</span> <span class="toc-text">全连接</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#卷积神经网络典型结构"><span class="toc-number">3.</span> <span class="toc-text">卷积神经网络典型结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#AlexNet"><span class="toc-number">3.1.</span> <span class="toc-text">AlexNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#特点"><span class="toc-number">3.1.1.</span> <span class="toc-text">特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#结构"><span class="toc-number">3.1.2.</span> <span class="toc-text">结构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ZFNet"><span class="toc-number">3.2.</span> <span class="toc-text">ZFNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VGG-卷积神经网络典型结构"><span class="toc-number">3.3.</span> <span class="toc-text">VGG 卷积神经网络典型结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GoogleNet"><span class="toc-number">3.4.</span> <span class="toc-text">GoogleNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ResNet"><span class="toc-number">3.5.</span> <span class="toc-text">ResNet</span></a></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://i.loli.net/2020/10/24/WZcoAJR2F4u38dt.png)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">Hexo</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad" data-src="/img/header.jpg" onerror="onerror=null;src='/img/friend_404.gif'"><div class="mobile_author-info__description"></div></div><hr><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">【神经网络】卷积神经网络</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2020-10-25<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2020-10-25</time></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h1><h2 id="卷积神经网络的应用"><a href="#卷积神经网络的应用" class="headerlink" title="卷积神经网络的应用"></a>卷积神经网络的应用</h2><p>计算机视觉（人脸识别、图像生成）自动驾驶等等</p>
<h2 id="传统神经网络-vs-卷积神经网络"><a href="#传统神经网络-vs-卷积神经网络" class="headerlink" title="传统神经网络 vs 卷积神经网络"></a>传统神经网络 vs 卷积神经网络</h2><p>损失函数：衡量吻合度（结果准确度）</p>
<p>传统神经网络的学习过程举例：</p>
<p><img src="/2020/10/25/【神经网络】卷积神经网络/1.PNG" alt></p>
<p>经过一次学习后，计算交叉熵（误差），根据 SGD随机梯度下降法进行学习纠正。</p>
<p>传统神经网络（全连接）面对参数较多的情况时，容易造成过拟合的现象。因此传统的神经网络应用到计算机视觉上时效率比较低。</p>
<p>这里假设输入的图像是300x300大小的。<br> 传统神经网络：假设我们用一个有128个单元的全连接层，则我们需要300x300x128=11520000个参数（不考虑偏置）。</p>
<p>卷积神经网络：假设我们采用5x5x3的filter，对于不同的区域，我们都共享同一个filter，因此就共享这同一组参数，一个filter有75个参数，假设我们使用10个filter，则需要750个参数（不考虑偏置）。</p>
<h1 id="基本组成结构"><a href="#基本组成结构" class="headerlink" title="基本组成结构"></a>基本组成结构</h1><p>一个典型的卷积网络是由卷积层、池化层、全连接层交叉堆叠而成。</p>
<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>概念：是对两个实变（以实数为自变量）函数的一种数学操作；滤波器/卷积核。</p>
<p>例如，在图像处理中，图像以二维矩阵形式输入神经网络，因此我们需要二维卷积。</p>
<p><img src="/2020/10/25/【神经网络】卷积神经网络/2.PNG" alt></p>
<p>给定一个图像 $X\in R^{M\times N}$ 和滤波器 $W\in R^{m\times n}$ ，计算 $y = Wx+b$ 。</p>
<p>卷积核中每个值为权重；每次卷积核进行计算时对应输入矩阵中的一块区域称为感受野；卷积后生成的结果称为特征图；深度（channel）指一层上卷积核个数。</p>
<p>padding：零填充</p>
<p><img src="/2020/10/25/【神经网络】卷积神经网络/3.PNG" alt></p>
<p>输出的特征图大小计算：</p>
<p><img src="/2020/10/25/【神经网络】卷积神经网络/5.PNG" alt></p>
<p>最后我的理解是，卷积的过程，就是从上一层数据中提取重要的特征数据送到下一层，可以用来减少参数值。</p>
<h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2><p>保留了主要特征的同时减少参数和计算量，防止过拟合，提高模型泛化能力。一般处于卷积层与卷积层之间，全连接层与全连接层之间。</p>
<ul>
<li><p>最大值池化（分类识别时常用）</p>
<p><img src="/2020/10/25/【神经网络】卷积神经网络/6.PNG" alt></p>
</li>
<li><p>平均池化</p>
<p><img src="/2020/10/25/【神经网络】卷积神经网络/7.PNG" alt></p>
</li>
</ul>
<h2 id="全连接"><a href="#全连接" class="headerlink" title="全连接"></a>全连接</h2><ul>
<li>两层之间所有神经元都有权重相连</li>
<li>通常全连接层在卷积神经网络尾部</li>
<li>全连接层通常参数量最大</li>
</ul>
<h1 id="卷积神经网络典型结构"><a href="#卷积神经网络典型结构" class="headerlink" title="卷积神经网络典型结构"></a>卷积神经网络典型结构</h1><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><p><img src="/2020/10/25/【神经网络】卷积神经网络/8.PNG" alt></p>
<p>关于RELU激活函数的特点之前总结过了，它能解决正区间内梯度消失的问题，且计算速度更快，收敛速度快。</p>
<p>Dropout 随机失活：训练时随机关闭部分神经元，测试时整合所有神经元，来防止过拟合。</p>
<p>数据增强：</p>
<ul>
<li>平移、翻转、对称<ul>
<li>随机crop。训练时，对于256 <em> 256的图片进行随机crop到224 </em> 224。</li>
<li>水平翻转，相当于将样本倍增</li>
</ul>
</li>
<li>改变RGB通道强度<ul>
<li>对RGB空间做一个高斯扰动（体现为改变照片中猫咪的颜色）</li>
</ul>
</li>
</ul>
<h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p><img src="/2020/10/25/【神经网络】卷积神经网络/9.PNG" alt></p>
<p>第一到五层：卷积—ReLU—池化</p>
<p>第六、七层：全连接—RELU—DropOut</p>
<p>第八层：全连接—SoftMax</p>
<h2 id="ZFNet"><a href="#ZFNet" class="headerlink" title="ZFNet"></a>ZFNet</h2><p>网络结构与AlexNet相同，只是将步长和感受野减小，以求得更加精确的结果。</p>
<h2 id="VGG-卷积神经网络典型结构"><a href="#VGG-卷积神经网络典型结构" class="headerlink" title="VGG 卷积神经网络典型结构"></a>VGG 卷积神经网络典型结构</h2><p>网络更深</p>
<h2 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h2><p><img src="/2020/10/25/【神经网络】卷积神经网络/10.PNG" alt></p>
<p>可以通过多卷积核增加特征多样性。</p>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>残差学习网络，深度达到了152层。</p>
<p>占坑，想后面详细了解下。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">iwannaeat</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://iwannaeat.github.io/2020/10/25/【神经网络】卷积神经网络/">https://iwannaeat.github.io/2020/10/25/【神经网络】卷积神经网络/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/神经网络/">神经网络    </a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/10/24/WZcoAJR2F4u38dt.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-left"><a href="/2020/11/05/【机器学习】有关范数/"><img class="prev_cover lozad" data-src="https://i.loli.net/2020/11/19/xeiV7gasZt62pdG.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Previous Post</div><div class="prev_info"><span>【机器学习】有关范数</span></div></a></div><div class="next-post pull-right"><a href="/2020/10/24/【神经网络】深度学习的数学基础/"><img class="next_cover lozad" data-src="https://i.loli.net/2020/10/24/WZcoAJR2F4u38dt.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>【神经网络】深度学习的数学基础</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/10/24/【神经网络】深度学习的数学基础/" title="【神经网络】深度学习的数学基础"><img class="relatedPosts_cover lozad" data-src="https://i.loli.net/2020/10/24/WZcoAJR2F4u38dt.png"><div class="relatedPosts_title">【神经网络】深度学习的数学基础</div></a></div></div><div class="clear_both"></div></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2021 By iwannaeat</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="Read Mode"> </i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion">繁</a><i class="fa fa-moon-o nightshift" id="nightshift" title="Dark Mode"></i></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script></body></html>