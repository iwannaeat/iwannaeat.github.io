<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>【多智能体】MADDPG算法 | Hexo</title><meta name="description" content="针对多智能体系统的强化学习方法 Multi-Agent Deep Deterministic Policy Gradient"><meta name="keywords" content="多智能体"><meta name="author" content="iwannaeat"><meta name="copyright" content="iwannaeat"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="https://iwannaeat.github.io/2021/04/22/【多智能体】MADDPG算法/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="【多智能体】MADDPG算法"><meta name="twitter:description" content="针对多智能体系统的强化学习方法 Multi-Agent Deep Deterministic Policy Gradient"><meta name="twitter:image" content="https://i.loli.net/2021/04/22/Blyqpd5tY6vwEhZ.png"><meta property="og:type" content="article"><meta property="og:title" content="【多智能体】MADDPG算法"><meta property="og:url" content="https://iwannaeat.github.io/2021/04/22/【多智能体】MADDPG算法/"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="针对多智能体系统的强化学习方法 Multi-Agent Deep Deterministic Policy Gradient"><meta property="og:image" content="https://i.loli.net/2021/04/22/Blyqpd5tY6vwEhZ.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="next" title="【强化学习】Policy-Gradient" href="https://iwannaeat.github.io/2021/02/19/【强化学习】Policy-Gradients/"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"cookieDomain":"https://iwannaeat.github.io/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Bookmark',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days'

  
}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#简介"><span class="toc-number">1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Policy-Gradient"><span class="toc-number">2.</span> <span class="toc-text">Policy Gradient</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Deterministic-Policy-Gradient"><span class="toc-number">3.</span> <span class="toc-text">Deterministic Policy Gradient</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DDPG"><span class="toc-number">4.</span> <span class="toc-text">DDPG</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://i.loli.net/2021/04/22/Blyqpd5tY6vwEhZ.png)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">Hexo</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad" data-src="/img/header.jpg" onerror="onerror=null;src='/img/friend_404.gif'"><div class="mobile_author-info__description"></div></div><hr><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">【多智能体】MADDPG算法</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2021-04-22<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2021-04-26</time></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>PG -&gt; DPG -&gt; DDPG-&gt;MADDPG</p>
<p>算法的特点是：</p>
<ul>
<li>其基本结构是AC+DDPG</li>
<li>每个智能体独立采样，可以有自己的奖励函数。合作和竞争环境中都可以使用</li>
<li>动作空间可连续</li>
</ul>
<h1 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h1><p>在之前的博客中已经对Policy Gradient的算法原理做了总结，包括其算法推导，如何进行梯度下降等等问题。接下来研究随即策略和确定性策略这两种算法</p>
<h1 id="Deterministic-Policy-Gradient"><a href="#Deterministic-Policy-Gradient" class="headerlink" title="Deterministic Policy Gradient"></a>Deterministic Policy Gradient</h1><p>确定性策略是和随机策略相对而言的，对于某一些动作集合来说，它可能是连续值，或者非常高维的离散值，这样动作的空间维度极大。如果我们使用随机策略，即像DQN一样研究它所有的可能动作的概率，并计算各个可能的动作的价值的话，那需要的样本量是非常大才可行的。于是有人就想出使用确定性策略来简化这个问题。</p>
<p>作为随机策略，在相同的策略，在同一个状态处，采用的动作是基于一个概率分布的，即是不确定的。而确定性策略则决定简单点，虽然在同一个状态处，采用的动作概率不同，但是最大概率只有一个，如果我们只取最大概率的动作，去掉这个概率分布，那么就简单多了。即作为确定性策略，相同的策略，在同一个状态处，动作是唯一确定的，即策略变成以下的公式：</p>
<script type="math/tex; mode=display">
\pi_\theta(s) = a</script><p>再对比之前Policy Gradient的策略梯度公式：</p>
<script type="math/tex; mode=display">
\nabla_\theta J(\theta)=E_{\tau\sim\pi_\theta(\tau)}[\nabla_\theta \log \pi_\theta(\tau) r(\tau)]</script><p>可以将这里的 $r(\tau)$ 写为 $Q_\pi(s,a)$ 的形式（加上衰减），可以看到这里的 $\tau$ 要在整个策略空间中进行采样，其中包括了动作和状态。</p>
<p>DPG基于Q值的确定性策略梯度的梯度计算公式是：</p>
<script type="math/tex; mode=display">
\nabla_\theta J(\theta)=E_{s\sim p_\pi}[\nabla_\theta \pi_\theta(s) \nabla_aQ_\pi(s,a)|a=\pi_\theta(s)]</script><p>跟随机策略梯度的式子相比，少了对动作的积分，多了回报Q函数对动作的导数</p>
<h1 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h1><p>DDPG中四个网络的功能如下：</p>
<ul>
<li>Actor当前网络：负责策略网络参数 $\theta$ 的迭代更新，负责根据当前状态 $S$ 选择当前动作 $A$，用于和环境交互生成 $S’$，$R$</li>
<li>Actor目标网络：负责根据经验回放池中采样的下一个状态 $S’$ 选择最优的下一个动作 $A’$ ，网络参数 $\theta’$ 定期从 $\theta$ 复制</li>
<li>Critic当前网络：负责价值网络参数w的迭代更新，负责计算当前Q值</li>
<li>Critic目标网络：负责计算目标Q值中的$Q′(S′,A′,w′)$部分。网络参数$w’$定期从$w$复制。</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">iwannaeat</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://iwannaeat.github.io/2021/04/22/【多智能体】MADDPG算法/">https://iwannaeat.github.io/2021/04/22/【多智能体】MADDPG算法/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/多智能体/">多智能体    </a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/04/22/Blyqpd5tY6vwEhZ.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2021/02/19/【强化学习】Policy-Gradients/"><img class="next_cover lozad" data-src="https://i.loli.net/2020/11/19/xeiV7gasZt62pdG.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>【强化学习】Policy-Gradient</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"></div><div class="clear_both"></div></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2021 By iwannaeat</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="Read Mode"> </i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion">繁</a><i class="fa fa-moon-o nightshift" id="nightshift" title="Dark Mode"></i></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script></body></html>