<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>【强化学习】马尔可夫决策过程 | Hexo</title><meta name="description" content="强化学习的基础MDPs。基于David Silver的强化学习课程总结。"><meta name="keywords" content="强化学习"><meta name="author" content="iwannaeat"><meta name="copyright" content="iwannaeat"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="https://iwannaeat.github.io/2020/12/02/【强化学习】马尔可夫决策过程/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="【强化学习】马尔可夫决策过程"><meta name="twitter:description" content="强化学习的基础MDPs。基于David Silver的强化学习课程总结。"><meta name="twitter:image" content="https://i.loli.net/2020/11/19/xeiV7gasZt62pdG.png"><meta property="og:type" content="article"><meta property="og:title" content="【强化学习】马尔可夫决策过程"><meta property="og:url" content="https://iwannaeat.github.io/2020/12/02/【强化学习】马尔可夫决策过程/"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="强化学习的基础MDPs。基于David Silver的强化学习课程总结。"><meta property="og:image" content="https://i.loli.net/2020/11/19/xeiV7gasZt62pdG.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="next" title="【强化学习】Q-Learning" href="https://iwannaeat.github.io/2020/11/26/【强化学习】Q-Learning/"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"cookieDomain":"https://iwannaeat.github.io/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Bookmark',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days'

  
}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#马尔科夫性"><span class="toc-number">1.1.</span> <span class="toc-text">马尔科夫性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#状态转移矩阵"><span class="toc-number">1.2.</span> <span class="toc-text">状态转移矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#片段（episode）"><span class="toc-number">1.3.</span> <span class="toc-text">片段（episode）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#马尔可夫过程"><span class="toc-number">1.4.</span> <span class="toc-text">马尔可夫过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#马尔科夫链"><span class="toc-number">1.5.</span> <span class="toc-text">马尔科夫链</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#生成模式（Generating-Patterns）"><span class="toc-number">1.6.</span> <span class="toc-text">生成模式（Generating Patterns）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#确定性模式（Deterministic-Patterns）：确定性系统"><span class="toc-number">1.6.1.</span> <span class="toc-text">确定性模式（Deterministic Patterns）：确定性系统</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#非确定性模式（Non-deterministic-patterns）：马尔可夫"><span class="toc-number">1.6.2.</span> <span class="toc-text">非确定性模式（Non-deterministic patterns）：马尔可夫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#隐藏模式（Hidden-Patterns）：隐马尔可夫"><span class="toc-number">1.6.3.</span> <span class="toc-text">隐藏模式（Hidden Patterns）：隐马尔可夫</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#马尔可夫决策过程"><span class="toc-number">2.</span> <span class="toc-text">马尔可夫决策过程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#马尔可夫奖励过程"><span class="toc-number">2.1.</span> <span class="toc-text">马尔可夫奖励过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#定义"><span class="toc-number">2.1.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#回报（收获）-G-t"><span class="toc-number">2.1.2.</span> <span class="toc-text">回报（收获）$G_{t}$</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#状态价值函数"><span class="toc-number">2.1.3.</span> <span class="toc-text">状态价值函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#贝尔曼方程"><span class="toc-number">2.1.4.</span> <span class="toc-text">贝尔曼方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#收获和价值的计算（例子）"><span class="toc-number">2.1.5.</span> <span class="toc-text">收获和价值的计算（例子）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bellman方程的矩阵形式和求解"><span class="toc-number">2.1.6.</span> <span class="toc-text">Bellman方程的矩阵形式和求解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#马尔可夫决策过程-1"><span class="toc-number">2.2.</span> <span class="toc-text">马尔可夫决策过程</span></a></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://i.loli.net/2020/11/19/xeiV7gasZt62pdG.png)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">Hexo</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad" data-src="/img/header.jpg" onerror="onerror=null;src='/img/friend_404.gif'"><div class="mobile_author-info__description"></div></div><hr><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">【强化学习】马尔可夫决策过程</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2020-12-02<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2020-12-03</time></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h2 id="马尔科夫性"><a href="#马尔科夫性" class="headerlink" title="马尔科夫性"></a>马尔科夫性</h2><p><strong>未来只与现在有关，与过去无关。</strong></p>
<p>设 $\{X(t),t\in T\}$ 为一随机过程，$E$ 为其状态空间，若对任意的 $t_1&lt;t_2&lt;…&lt;t_n&lt;t$ ，任意的 $x_1,x_2,…,x_n,x\in E$ ，随机变量 $X(t)$ 在已知变量 $X(t_1)=x_1,…,X(t_n)=x_n$ 之下的条件分布函数只与 $X(t_n) = x_n$ 有关，而与 $X(t_1)=x_1,…,X(t_{n-1})=x_{n-1}$ 无关，即条件分布函数满足等式：</p>
<script type="math/tex; mode=display">
F(x,t|x_n,x_{n-1},...,x_2,x_1,t_n,t_{n-1},...,t_2,t_1)=F(x,t|x_n,t_n)</script><p>此性质被称为马尔科夫性，也称为无后效性或无记忆性。</p>
<p>若 $X(t)$ 为离散型随机变量，则马尔科夫性亦满足等式：</p>
<script type="math/tex; mode=display">
P\{X(t) = x|X(t_n) = x_n,...,X(t_1)=x_1\}=P\{X(t)=x|X(t_n)=x_n\}</script><p>具有马尔科夫性的随机过程 $\{X(t),t\in T\}$ 被称为马尔可夫过程。</p>
<p>具有马尔可夫性质的过程满足下列公式：</p>
<script type="math/tex; mode=display">
P(S_{t+1}|S_t)=P(S_{t+1}|S_1,...,S_t)</script><p>即：</p>
<ul>
<li>某个给定的当前状态 $S_t$ ，其将来的状态 $S_{t+1}$ 与 $t$ 时刻之前的状态都已经没有关系。</li>
<li>状态 $S_t$ 包含了所有历史相关信息，或者说历史的所有状态的相关信息都在当前状态 $S_t$ 上体现出来，一旦 $S_t$ 知道了，那么 $S_1,S_2,…,S_{t-1}$ 都可以被抛弃。数学上可以认为：<strong>状态是将来的充分统计量</strong>（即凭借当前状态能够唯一的确定之后时刻的状态）。因此，这里要求<strong>环境全观测</strong>。</li>
</ul>
<p>举个例子：比如说下围棋时，关注的只是当前的棋盘局面。</p>
<h2 id="状态转移矩阵"><a href="#状态转移矩阵" class="headerlink" title="状态转移矩阵"></a>状态转移矩阵</h2><p>状态转移概率：从一个马尔可夫状态 $s$ 跳转到后继状态 $s’$ 的概率（空心字母表示统计运算符）</p>
<script type="math/tex; mode=display">
\mathcal{P}_{ss'}=\mathbb{P}[S_{t+1}=s'|S_t=s]</script><p>将所有的状态 $s$ 组成行，后继状态 $s’$ 组成列，得到状态转移矩阵：</p>
<script type="math/tex; mode=display">
\mathcal{P}=\begin{bmatrix}
\mathcal{P}_{11}      & \cdots & \mathcal{P}_{1n}     \\
\vdots & \ddots & \vdots \\
\mathcal{P}_{n1} & \cdots & \mathcal{P}_{nn}
\end{bmatrix}</script><p>其中：</p>
<ul>
<li>n 表示状态的个数</li>
<li>$\mathcal{P}$ 代表整个状态转移的集合</li>
<li><p>每行元素相加等于 1</p>
<p>当状态数量太多或者是无穷多状态（连续状态）时，更适合使用状态转移函数，此时有</p>
<script type="math/tex; mode=display">
\int_{s'}\mathcal{P}(s'|s)=1</script></li>
</ul>
<h2 id="片段（episode）"><a href="#片段（episode）" class="headerlink" title="片段（episode）"></a>片段（episode）</h2><p>强化学习中，从初始状态 $S_1$ 到终止状态 $S_T$ 的序列过程，被称为一个片段：$S_1,S_2,…,S_T$ 。</p>
<p>如果一个任务总<strong>以终止状态结束</strong>那么这个任务被称为<strong>片段任务</strong>（episodic task）。</p>
<p>如果一个任务<strong>没有终止状态</strong>，会被无限执行下去，这被称为<strong>连续性任务</strong>（continuing task）。</p>
<h2 id="马尔可夫过程"><a href="#马尔可夫过程" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h2><p>若随机过程 $\{X(t),t\in T\}$ 满足马尔科夫性，则称为马尔可夫过程。</p>
<p>一个马尔可夫过程是一个<strong>无记忆的随机过程</strong>（我的理解：无记忆，就是说只需要知道当前状态，而不需要模型去记忆历史状态；随机过程即概率论的知识，即从当前状态转移到哪一个新状态是随机的，与转移矩阵不矛盾）。</p>
<p>马尔可夫过程可以由一个二元组来定义：$<s,\mathcal{p}>$，其中，$S$ 代表了状态的集合，$\mathcal{P}$ 描述了状态转移矩阵：$\mathcal{P}_{ss’}=\mathbb{P}[S_{t+1}=s’|S_t=s]$ 。</s,\mathcal{p}></p>
<p>有时我们并不知道 $\mathcal{P}$ 的具体值，但我们假设 $\mathcal{P}$ 是存在且稳定的；当 $\mathcal{P}$ 不稳定时，意味着模型需要从新的环境中获取信息，更新状态转移矩阵，这时就涉及到在线学习了。</p>
<h2 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h2><p>即状态空间为可数集的马尔可夫过程。</p>
<h2 id="生成模式（Generating-Patterns）"><a href="#生成模式（Generating-Patterns）" class="headerlink" title="生成模式（Generating Patterns）"></a>生成模式（Generating Patterns）</h2><h3 id="确定性模式（Deterministic-Patterns）：确定性系统"><a href="#确定性模式（Deterministic-Patterns）：确定性系统" class="headerlink" title="确定性模式（Deterministic Patterns）：确定性系统"></a>确定性模式（Deterministic Patterns）：确定性系统</h3><p>举例：交通信号灯：红-绿-黄-红……</p>
<p>已知当前状态是绿色，那么下一个状态一定是黄色。也就是说，该系统是确定的。</p>
<h3 id="非确定性模式（Non-deterministic-patterns）：马尔可夫"><a href="#非确定性模式（Non-deterministic-patterns）：马尔可夫" class="headerlink" title="非确定性模式（Non-deterministic patterns）：马尔可夫"></a>非确定性模式（Non-deterministic patterns）：马尔可夫</h3><p>例如：预测天气。</p>
<p><strong>马尔科夫假设</strong>：假设模型的当前状态仅仅依赖于前面的几个状态。</p>
<p>虽然在将模型简化为马尔可夫过程的时候，忽略了一些环境中的重要因素（此例子中包括风力、气压等等），但是这样简化的模型易于求解，所以我们通常接受这样的模型假设。</p>
<p><strong>n 阶马尔可夫模型</strong>：下一个状态选择取决于前 n 个状态。</p>
<p>最简单的马尔可夫过程是一阶模型，其状态选择仅与前一个状态有关。对于有 M 个状态的一阶马尔可夫模型，共有 $M^2$ 个状态转移，每个状态转移都有一个概率值，称为状态转移概率。这样的 $M^2$ 个概率也可以用一个状态转移矩阵来表示，注意这些<strong>概率并不随着时间的变化而变化</strong>（非常重要但有时不符合实际的假设）。</p>
<h3 id="隐藏模式（Hidden-Patterns）：隐马尔可夫"><a href="#隐藏模式（Hidden-Patterns）：隐马尔可夫" class="headerlink" title="隐藏模式（Hidden Patterns）：隐马尔可夫"></a>隐藏模式（Hidden Patterns）：隐马尔可夫</h3><p>没懂，暂时不需要，占坑。</p>
<h1 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h1><p><strong>完全可观测</strong>：个体能够直接观测得到所有需要的环境状态。在这种条件下，个体对环境的观测=个体状态=环境状态。</p>
<p>MDP 是对完全可观测（Fully observable）的环境进行描述的，即观测到的状态内容完整的决定了决策需要的特征；几乎所有的强化学习问题都可以转化为 MDP。</p>
<h2 id="马尔可夫奖励过程"><a href="#马尔可夫奖励过程" class="headerlink" title="马尔可夫奖励过程"></a>马尔可夫奖励过程</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>是一个带有价值的马尔可夫过程，可以表示为 $\{S,\mathcal{P},\mathcal{R},\gamma\}$ ，其中：</p>
<ul>
<li>S 为有限状态的集合</li>
<li>P 为状态转移矩阵</li>
<li>R 为奖励函数，描述了在状态 s 时的奖励，$\mathcal{R}_s=E[R_{t+1}|S_t=s]$</li>
<li>$\gamma$ 为衰减系数</li>
</ul>
<p>这里的奖励函数为什么是 $R_{t+1}$ 起初不是很理解，在查了资料之后，我的理解如下：广义上的期望指的是均值，也就是说这个值在模型中应该是事先通过各种测量和观察得到的给定的值，而不是在模型中进行计算的（问问老师）。</p>
<p>而这里取 $R_{t+1}$ 的理由，可以理解为是一个约定，指的是离开状态 $S_t=s$ 能够获得的立即奖励。</p>
<h3 id="回报（收获）-G-t"><a href="#回报（收获）-G-t" class="headerlink" title="回报（收获）$G_{t}$"></a>回报（收获）$G_{t}$</h3><p>$G_t$ 是从时间序列 $t$ 开始所有的折扣回报（乘以 $\gamma$ ）</p>
<ul>
<li>针对连续性任务而言（无穷个状态），$G_t=R_{t+1} +\gamma R_{t+2}+…=\sum_{k=0}^\infin\gamma^kR_{t+k+1}$</li>
<li>针对片段性任务而言（有限个状态），$G_t=R_{t+1} +\gamma R_{t+2}+…+\gamma^{T-t-1}R_T=\sum_{k=0}^{T-t-1}\gamma^kR_{t+k+1}$</li>
</ul>
<p>上述片段性任务的终止状态是时间到达 T。</p>
<p>奖励和回报的不同：奖励是针对状态的，即转移到下一个状态会得到的奖励；回报是立足现在对未来某种状态序列的评估，是从 t 时刻开始往后所有的奖励的有衰减的收益总和，是针对一个片段的。</p>
<p>由于无法穷尽所有可能的序列，所以并不能精确算出所有 $G_t$。</p>
<h3 id="状态价值函数"><a href="#状态价值函数" class="headerlink" title="状态价值函数"></a>状态价值函数</h3><p>是从状态 s 开始的回报的期望值：$v(s)=E[G_t|S_t=s]$</p>
<h3 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h3><p>公式及推导：</p>
<script type="math/tex; mode=display">
\begin{aligned}
v(s)&=E[G_t|S_t=s]
    \\&=E[R_{t+1}+\gamma G_{t+1}|S_t=s]
    \\&=E[R_{t+1}|S_t=s]+E[\gamma G_{t+1}|S_t=s]
    \\&=R_{t+1}+\gamma v(S_{t+1})
    \\&=E[R_{t+1}|S_t=s]+E[\gamma v(S_{t+1})|S_t=s]
    \\&=E[R_{t+1}+\gamma v(S_{t+1})|S_t=s]
    \end{aligned}</script><p>去掉期望：假设下一个状态是 $s’$，即 $s’=S_{t+1}$，那么有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
v(s)&=E[R_{t+1}+\gamma v(s')|S_t=s]
\\&=R_s+\gamma \sum_{s'\in S}P_{ss'}v(s')
\end{aligned}</script><p>$R_s$ 指的是离开当前状态 $s$ 所获得的奖励。</p>
<p>对于大规模的 MRP 问题，通常采用动态规划、蒙特卡洛估计等等方法来迭代求解。</p>
<h3 id="收获和价值的计算（例子）"><a href="#收获和价值的计算（例子）" class="headerlink" title="收获和价值的计算（例子）"></a>收获和价值的计算（例子）</h3><p>在马尔可夫过程上，添加了对每一个状态的奖励：</p>
<p><img src="/2020/12/02/【强化学习】马尔可夫决策过程/v2-3f429cd0a2cb758aa04058d2cdcb0fc5_r.jpg" alt="v2-3f429cd0a2cb758aa04058d2cdcb0fc5_r" style="zoom:30%;"></p>
<p>可以将图中给出的条件表示成下面的矩阵形式：<br><img src="/2020/12/02/【强化学习】马尔可夫决策过程/捕获.PNG" alt="捕获"></p>
<p>假设得到了下面4个马尔科夫链（四条达到终点的路径），假设 $\gamma=0.5$，出发点为 $C_1$，可以求出每条马尔可夫链的 G 值。</p>
<p><img src="/2020/12/02/【强化学习】马尔可夫决策过程/20180116225052954.png" alt="20180116225052954"></p>
<p>再举个计算状态价值函数的例子：（$\gamma = 1$）</p>
<p><img src="/2020/12/02/【强化学习】马尔可夫决策过程/v2-82b3dfa8a19f9054ebe976f2194f2081_hd.jpg" alt="v2-82b3dfa8a19f9054ebe976f2194f2081_hd"></p>
<p>在状态class3处的状态值函数计算就非常简单了。这里还是需要说明：其实一开始计算时候0.8肯定是不存在的，实际计算可以采用迭代法，一开始任意初始化，后面学习更新即可，这就是所谓的值迭代法。当然对于这个问题，可以采用下面的方法直接求解。</p>
<h3 id="Bellman方程的矩阵形式和求解"><a href="#Bellman方程的矩阵形式和求解" class="headerlink" title="Bellman方程的矩阵形式和求解"></a>Bellman方程的矩阵形式和求解</h3><p><img src="/2020/12/02/【强化学习】马尔可夫决策过程/v2-d4cabfb6160ddf725b74bb35180401bc_hd.jpg" alt="v2-d4cabfb6160ddf725b74bb35180401bc_hd"></p>
<p>将上述的不含期望的贝尔曼期望方程写成矩阵形式，可以看出它是一个线性方程，那么可以直接采用线性代数的方法求解，无需迭代。</p>
<p>求解后得到的结果是：$v=(I-\gamma P)^{-1}R$</p>
<p>实际上，<strong>计算复杂度是 $O(n^3)$</strong>，<img src="https://www.zhihu.com/equation?tex=n" alt="[公式]">是状态数量。因此直接求解仅适用于小规模的MRPs。大规模MRP的求解通常使用迭代法。常用的迭代方法有：动态规划（Dynamic Programming, DP）、蒙特卡洛评估（Monte-Carlo  evaluation）、时序差分学习（Temporal-Difference, TD）。</p>
<h2 id="马尔可夫决策过程-1"><a href="#马尔可夫决策过程-1" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h2></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">iwannaeat</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://iwannaeat.github.io/2020/12/02/【强化学习】马尔可夫决策过程/">https://iwannaeat.github.io/2020/12/02/【强化学习】马尔可夫决策过程/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/强化学习/">强化学习    </a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/11/19/xeiV7gasZt62pdG.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2020/11/26/【强化学习】Q-Learning/"><img class="next_cover lozad" data-src="https://i.loli.net/2020/11/19/xeiV7gasZt62pdG.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>【强化学习】Q-Learning</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/11/26/【强化学习】Q-Learning/" title="【强化学习】Q-Learning"><img class="relatedPosts_cover lozad" data-src="https://i.loli.net/2020/11/19/xeiV7gasZt62pdG.png"><div class="relatedPosts_title">【强化学习】Q-Learning</div></a></div><div class="relatedPosts_item"><a href="/2020/11/26/【强化学习】简介/" title="【强化学习】简介"><img class="relatedPosts_cover lozad" data-src="https://i.loli.net/2020/11/19/xeiV7gasZt62pdG.png"><div class="relatedPosts_title">【强化学习】简介</div></a></div></div><div class="clear_both"></div></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2020 By iwannaeat</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="Read Mode"> </i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion">繁</a><i class="fa fa-moon-o nightshift" id="nightshift" title="Dark Mode"></i></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script></body></html>